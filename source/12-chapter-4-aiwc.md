#AIWC: OpenCL based Architecture Independent Workload Characterization

In this chapter we present the Architecture Independent Workload Characterization (AIWC) tool.
AIWC simulates the execution of OpenCL kernels to collect architecture-independent features that characterize each code, which may also be used in performance prediction.

AIWC verifies the architecture independent metrics since they are collected on a toolchain and in a language actively executed on a wide range of accelerators -- the OpenCL runtime supports execution on CPU, GPU, DSP, FPGA, MIC and ASIC hardware architectures.
The intermediate representation of the OpenCL kernel code is a subset of LLVM IR known as SPIR -- Standard Portable Intermediate Representation.
This IR forms a basis for Oclgrind to perform OpenCL device simulation, which interprets LLVM IR instructions.

Migrating the metrics presented in the ISA-independent workload characterization paper [@shao2013isa] to the Oclgrind tool offers an accessible, high-accuracy and reproducible method to acquire these AIWC features.
Namely:

* Accessibility: since the Oclgrind OpenCL kernel debugging tool is one of the most adopted OpenCL debugging tools freely available to date, having AIWC metric generation included as an Oclgrind plugin allows rapid workload characterization.
* High-Accuracy: evaluating the low level optimized IR does not suffer from a loss of precision since each instruction is instrumented during its execution in the simulator, unlike with the conventional metrics generated by measuring architecture driven events -- such as PAPI and MICA analysis.
* Reproducibility: each instruction is instrumented by the AIWC tool during execution, there is no variance in the metric results presented between OpenCL kernel runs.

The caveat with this approach is the overhead imposed by executing full solution HPC codes on a slower simulator device.
However, since AIWC metrics do not vary between runs, this is still a shorter execution time than the typical number of iterations required to get a reasonable statistical sample when compared to a MICA or architecture dependent analysis.

<!--
Oclgrind OpenCL kernel debugging tool is one of the most adopted OpenCL debugging tools freely available to date, and increases the accessibility of workload characterization -- by having these features as an instrumentation plugin.
-->


Application codes differ in resource requirements, control structure and available parallelism.
Similarly, compute devices differ in number and capabilities of execution units, processing model, and available resources.
Given performance measurements for particular combinations of codes and devices, it is difficult to generalize to novel combinations.
Hardware designers and HPC integrators would benefit from accurate and systematic performance prediction, for example, in designing an HPC system, to choose a mix of accelerators that are well-suited to the expected workload.

Measuring performance-critical characteristics of application workloads is important both for developers, who must understand and optimize the performance of codes, as well as designers and integrators of HPC systems, who must ensure that compute architectures are suitable for the intended workloads.
However, if these workload characteristics are tied to architectural features that are specific to a particular system, they may not generalize well to alternative or future systems.

An architecture-independent method ensures an accurate characterization of inherent program behaviour, without bias due to architecture-dependent features that vary widely between different types of accelerators.

AIWC is the first workload characterization tool to support multi-threaded or parallel workloads, which it achieves by collecting metrics that indicate both instruction and thread-level parallelism.
We demonstrate the use of AIWC to characterize a variety of codes in the Extended OpenDwarfs Benchmark Suite [@johnston18opendwarfs] -- presented in chapter 3.
We begin with an introduction of the metrics collected by AIWC, we then discuss how AIWC was implemented and demonstrate its usage on the \texttt{lud}, `nw`, `swat`, `gem`, `kmeans` and `hmm` benchmarks.
Finally, we conclude with a summary of what AIWC and the associated metrics provide to prediction.
A majority of this Chapter was published in the LLVM-HPC workshop proceedings as part of the 30th International Conference for High Performance Computing, Networking, Storage, and Analysis (SC18) 2018 [@aiwc2018].
<!--
Additionally, work from [Section @sec:case-study-bio] has been submitted as as Special Issue paper in the International Journal of High Performance Computing Applications (IJHPCA) and is currently under review.-->

## Metrics

For each OpenCL kernel invocation, the Oclgrind simulator **AIWC** tool collects a set of metrics, which are listed in [Table @tbl:aiwc-metrics].
Exploitable coarse-grained parallelism is measured by counting the number of work-items and barriers encountered.
Instructions To Barrier (ITB) and Instructions per Thread (IPT) can be used to indicate workload irregularity or imbalance.


\begin{table*}[t]
\caption{Metrics collected by the \textbf{AIWC} tool ordered by type. \label{tbl:aiwc-metrics}}

\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}cll@{}}
\toprule

{Type} & {Metric} & {Description}\\\hline

Compute & Opcode & total \# of unique opcodes required to cover 90\% of dynamic
instructions\\
Compute & Total Instruction Count & total \# of instructions executed\\
Parallelism & Work-items & total \# of work-items or threads executed\\
Parallelism & Total Barriers Hit & total \# of barrier instructions\\
Parallelism & Min ITB & minimum \# of instructions executed until a barrier\\
Parallelism & Max ITB & maximum \# of instructions executed until a barrier\\
Parallelism & Median ITB & median \# of instructions executed until a barrier\\
Parallelism & Min IPT & minimum \# of instructions executed per thread\\
Parallelism & Max IPT & maximum \# of instructions executed per thread\\
Parallelism & Median IPT & median \# of instructions executed per thread\\
Parallelism & Max SIMD Width & maximum \# of data items operated on during an instruction\\
Parallelism & Mean SIMD Width & mean \# of data items operated on during an instruction\\
Parallelism & SD SIMD Width & standard deviation across \# of data items affected\\
Memory & Total Memory Footprint & total \# of unique memory addresses accessed\\
Memory & 90\% Memory Footprint & \# of unique memory addresses that cover 90\% of memory accesses\\
Memory & Unique Reads & total \# of unique memory addresses read\\
Memory & Unique Writes & total \# of unique memory addresses written\\
Memory & Unique Read/Write Ratio & indication of workload being (unique reads / unique writes) \\
Memory & Total Reads & total \# of memory addresses read\\
Memory & Total Writes & total \# of memory addresses written\\
Memory & Reread Ratio & indication of memory reuse for reads (unique reads/total reads)\\
Memory & Rewrite Ratio & indication of memory reuse for writes (unique writes/total writes)\\
Memory & Global Memory Address Entropy & measure of the randomness of memory addresses\\
Memory & Local Memory Address Entropy & measure of the spatial locality of memory addresses\\
Control & Total Unique Branch Instructions & total \# of unique branch instructions\\
Control & 90\% Branch Instructions & \# of unique branch instructions that cover 90\%
of branch instructions\\
Control & Yokota Branch Entropy & branch history entropy using Shannon's information entropy\\
Control & Average Linear Branch Entropy & branch history entropy score using the
average linear branch entropy\\
\hline
\end{tabular}
}
\end{table*}

The **Opcode**, **total memory footprint** and **90% memory footprint** measures are simple counts.
Likewise, **total instruction count** is the number of instructions achieved during a kernel execution.
The **global memory address entropy** is a positive real number that corresponds to the randomness of memory addresses accessed.
The **local memory address entropy** is computed as 10 separate values according to increasing number of Least Significant Bits (LSB), or low order bits, omitted in the calculation.
The number of bits skipped ranges from 1 to 10, and a steeper drop in entropy with increasing number of bits indicates greater spatial locality in the address stream.

Both **unique branch instructions** and the associated **90% branch instructions** are counts indicating the count of logical control flow branches encountered during kernel execution.
**Yokota branch entropy** ranges between 0 and 1, and offers an indication of a program's predictability as a floating point entropy value. [@yokota2007introducing] 
The **average linear branch entropy** metric is proportional to the miss rate in program execution; $p=0$ for branches always taken or not-taken but $p=0.5$ for the most unpredictable control flow.
All branch-prediction metrics were computed using a fixed history of 16-element branch strings, each of which is composed of 1-bit branch results (taken/not-taken).

As the OpenCL programming model is targeted at parallel architectures, any workload characterization must consider exploitable parallelism and associated communication and synchronization costs.
We characterize thread-level parallelism (TLP) by the number of **work-items** executed by each kernel, which indicates the maximum number of threads that can be executed concurrently.

Work-item communication hinders TLP, and in the OpenCL setting, takes the form of either local communication (within a work-group) using local synchronization (barriers) or globally by dividing the kernel and invoking the smaller kernels on the command queue.
Both local and global synchronization can be measured in **instructions to barrier** (ITB) by performing a running tally of instructions executed per work-item until a barrier is encountered under which the count is saved and resets; this count will naturally include the final (implicit) barrier at the end of the kernel.
**Min**, **max** and **median ITB** are reported to understand synchronization overheads, as a large difference between min and max ITB may indicate an irregular workload.

**Instructions per thread** (IPT) based metrics are generated by performing a running tally of instructions executed per work-item until completion.
The count is saved and resets.
**Min**, **max** and **median IPT** are reported to understand load imbalance.

To characterize data parallelism, we examine the number and width of vector operands in the generated LLVM IR, reported as **max SIMD width**, **mean SIMD width** and standard deviation -- **SD SIMD width**.
Further characterisation of parallelism is presented in the **work-items** and **total barriers hit** metrics.

Some of the other metrics are highly dependent on workload scale, so **work-items** may be used to normalize between different scales.
For example, **total memory footprint** can be divided by **work-items** to give the total memory footprint per work-item, which indicates the memory required per processing element.

Finally, unique verses absolute reads and writes can indicate shared and local memory reuse between work-items within a work-group, and globally, which shows the predictability of a workload.
To present these characteristics the **unique reads**, **unique writes**, **unique read/write ratio**, **total reads**, **total writes**, **reread ratio**, **rewrite ratio** metrics are proposed.
The **unique read/write ratio** shows that the workload is balanced, read intensive or write intensive.
They are computed by storing read and write memory accesses separately and are later combined, to compute the **global memory address entropy** and **local memory address entropy** scores.


## Implementation


AIWC is implemented as a plugin for Oclgrind, which simulates kernel execution on an ideal compute device.
OpenCL kernels are executed in series, and Oclgrind generates notification events which AIWC handles to populate data structures for each workload metric.
Once each kernel has completed execution, AIWC performs statistical summaries of the collected metrics by examining these data structures.

The **Opcode** diversity metric updates a counter on an unordered map during each \texttt{workItemBegin} event, the type of operation is determined by examining the opcode name using the LLVM Instruction API.

The number of **work-items** is computed by incrementing a global counter -- accessible by all work-item threads -- once a \texttt{workItemBegin} notification event occurs.

TLP metrics require barrier events to be instrumented within each thread.
Instructions To Barrier **ITB** metrics require each thread to increment a local counter once every \texttt{instructionExecuted} has occurred, this counter is added to a vector and reset once the work-item encounters a barrier.
The **Total Barriers Hit** counter also increments on the same condition.
Work-items are executed sequentially within all work-items in a work-group.
If a barrier is hit the queue moves onto all other available work-items in a ready state.
Collection of the metrics post barrier resumes during the \texttt{workItemClearBarrier} event.

ILP **SIMD** metrics examine the size of the result variable provided from the \texttt{instructionExecuted} notification, the width is then added to a vector for the statistics to be computed once the kernel execution has completed.

**Total Memory Footprint** **90% Memory Footprint** and Local Memory Address Entropy **LMAE** metrics require the address accessed to be stored during kernel execution and occurs during the \texttt{memoryLoad}, \texttt{memoryStore}, \texttt{memoryAtomicLoad} and \texttt{memoryAtomicStore} notifications.

Branch entropy measurements require a check during \texttt{instructionExecuted} event on whether the instruction is a branch instruction, if so a flag indicating a branch operation has occurred is set and both LLVM IR labels -- which correspond to branch targets -- are recorded.
On the next \texttt{instructionExecuted} the flag is queried and reset while the current instruction label is compared against which of the two targets were taken, the result is stored in the branch history trace.
The implementation of this is shown in Listing \ref{lst:instructionExecuted}.
Note the `instructionExecuted` callback is propagated from Oclgrind during every OpenCL kernel instruction -- emulated in LLVM IR.
This function also updates variables to track instruction diversity by counting the occurrences of each instruction, instructions to barrier and other parallelism metrics by running a counter until a barrier is hit, finally, the vectorization -- as part of the parallelism metrics -- are updated by recording the width of executed instructions.
The `m_state` variable is shared between all work-items in a work-group and these are stored into a global set of variables using a mutex lock once the work-group has completed execution.

The branch metrics are then computed by evaluating the full history of combined branch's taken and not-taken.

\begin{lstlisting}[float=tp,language=C++, caption={The Instruction Executed callback function collects specific program metrics and adds them to a history trace for later analysis.},label={lst:instructionExecuted},linewidth=\columnwidth,numbers=left, numberstyle=\small, numbersep=8pt, frame = single, breaklines=true]
void WorkloadCharacterisation::instructionExecuted(...,  const llvm::Instruction *instruction, ...){
    unsigned opcode = instruction->getOpcode();
    std::string opcode_name = llvm::Instruction::getOpcodeName(opcode);
    //update key-value pair of instruction name and its occurrence in the kernel
    (*m_state.computeOps)[opcode_name]++;
    std::string Str = "";
    //if a conditional branch which has labels, store the labels to track
    //in the next instruction which of the two lines we end up in
    if (opcode == llvm::Instruction::Br && instruction->getNumOperands() == 3){
        if(instruction->getOperand(1)->getType()->isLabelTy() &&
                instruction->getOperand(2)->getType()->isLabelTy()){
            m_state.previous_instruction_is_branch = true;
            llvm::raw_string_ostream OS(Str);
            instruction->getOperand(1)->printAsOperand(OS,false);
            m_state.target1 = Str;
            Str = "";
            instruction->getOperand(2)->printAsOperand(OS,false);
            m_state.target2 = Str;
            llvm::DebugLoc loc = instruction->getDebugLoc();
            m_state.branch_loc = loc.getLine();
         }
    }
    //if the last instruction was a branch, log which of the two targets were taken
    else if (m_state.previous_instruction_is_branch == true){
        llvm::raw_string_ostream OS(Str);
        instruction->getParent()->printAsOperand(OS,false);
        if(Str == m_state.target1)
            (*m_state.branchOps)[m_state.branch_loc].push_back(true);//taken
        else if(Str == m_state.target2){
            (*m_state.branchOps)[m_state.branch_loc].push_back(false);//not taken
        }
        m_state.previous_instruction_is_branch = false;
    }
    //counter for instructions to barrier and other parallelism metrics
    m_state.instruction_count++;
    m_state.workitem_instruction_count++;
    //SIMD instruction width metrics use the following
    m_state.instructionWidth->push_back(result.num);
\end{lstlisting}

The **Total Unique Branch Instructions** is a count of the absolute number of unique locations that branching occurred, while the **90% Branch Instructions** indicates the number of unique branch locations that cover 90% of all branches.
**Yokota** from Shao [@shao2013isa], and **Average Linear Branch Entropy**, from De Pestel [@depestel2017linear] and have been computed and are also presented based on this implementation.
\texttt{workGroupComplete} events trigger the collection of the intermediate work-item and work-group counter variables to be added to the global suite, while \texttt{workGroupBegin} events reset all the local/intermediate counters.

Finally, \texttt{kernelBegin} initializes the global counters and \texttt{kernelEnd} triggers the generation and presentation of all the statistics listed in [Table @tbl:aiwc-metrics].
The source code is available at the GitHub Repository [@beau_johnston_2017_1134175].


## Demonstration

We now demonstrate the use of **AIWC** on several scientific application kernels selected from the Extended OpenDwarfs Benchmark Suite [@johnston18opendwarfs].
These benchmarks were extracted from and are representative of general scientific application codes.
Our selection is not intended to be exhaustive, rather, it is meant to illustrate how key properties of the codes are reflected in the metrics collected by **AIWC**.

AIWC is run on full application codes, but it is difficult to present an entire summary due to the nature of OpenCL.
Computationally intensive kernels are simply selected regions of the full application codes and are invoked separately for device execution.
As such, the AIWC metrics can either be shown per kernel run on a device, or as the summation of all metrics for a kernel for a full application at a given problem size -- we chose the latter.
Additionally, given the number of kernels presented we believe AIWC will generalize to full codes in other domains.

We present metrics for 11 different application codes -- which includes 37 kernels in total.
Each code was run with four different problem sizes, called **tiny**, **small**, **medium** and **large** in the Extended OpenDwarfs Benchmark Suite; these correspond respectively to problems that would fit in the L1, L2 and L3 cache or main memory of a typical current-generation CPU architecture.
As simulation within Oclgrind is deterministic, all results presented are for a single run for each combination of code and problem size.

In a cursory breakdown, four selected metrics are presented in [Figure @fig:stacked_plots].
One metric was chosen from each of the main categories, namely, Opcode, Barriers Per Instruction, Global Memory Address Entropy, Branch Entropy (Linear Average).
Each category has also been segmented by colour: blue results represent *compute* metrics, green represent metrics that indicate *parallelism*, yellow represents *memory* metrics and purple bars represent *control* metrics.
Median results are presented for each metric -- while there is no variation between invocations of AIWC, certain kernels are iterated multiple times and over differing domains/data sets.
Each of the 4 sub-figures shows all kernels over the 4 different problem sizes.

For almost all benchmarks the global memory address entropy increases with problem size, whereas the other metrics do not increase.
Notably, memory entropy is low for \texttt{lud\_diagonal}, reflecting memory access with constant strides of diagonal matrix elements, and \texttt{cl\_fdt53Kernel}, again reflecting regular strides generated by downsampling in the discrete wavelet transform.
Note, we do not present **medium** and **large** problem sizes for some kernels due to various issues including: a lack of input datasets, failure of AIWC in tracing large numbers of memory and branch operations for entropy calculations.
These issues will be addressed in future work.

Looking at branch entropy, \texttt{bfs\_kernel2} stands out as having by far the greatest entropy.
This kernel is dominated by a single branch instruction based on a flag value which is entirely unpredictable, and could be expected to perform poorly on a SIMT architecture such as a GPU.

Barriers per instruction is quite low for most kernels, with the exception of \texttt{needle\_opencl\_shared\_1} and \texttt{needle\_opencl\_shared\_2} from the Needleman-Wunsch DNA sequence alignment dynamic programming benchmark.
These kernels each have 0.04 barriers per instruction (i.e. one barrier per 25 instructions), as they follow a highly-synchronized wavefront pattern through the matrix representing matching pairs.
The performance of this kernel on a particular architecture could be expected to be highly dependent on the cost of synchronization.

\begin{figure*}
\centering
\hspace*{-0.3cm}
\includegraphics[width=1.04\linewidth,keepaspectratio]{./figures/chapter-4/draw_stacked_plots-1.pdf}
\caption{Selected AIWC metrics from each category over all kernels and 4 problem sizes.}
\label{fig:stacked_plots} 
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=.72\columnwidth,keepaspectratio]{./figures/chapter-4/draw_lud_diagonal_internal_all_kiviat-1.pdf}
    \caption{A) and B) show the AIWC features of the \texttt{diagonal} and \texttt{internal} kernels of the LUD application over all problem sizes.}
    \label{fig:kiviat}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=.7\columnwidth,keepaspectratio]{./figures/chapter-4/draw_lud_diagonal_perimeter_lmae_all_kiviat-1.pdf}
    \caption{A) shows the AIWC features of the \texttt{perimeter} kernel of the LUD application over all problem sizes. B) shows the corresponding Local Memory Address Entropy for the \texttt{perimeter} kernel over the tiny problem size.}
    \label{fig:kiviat2}
\end{figure*}

## Detailed Analysis of LU Decomposition Benchmark

We now proceed with a more detailed investigation of one of the benchmarks, **lud**, which performs decomposition of a matrix into upper and lower triangular matrices.
Following Shao and Brooks [@shao2013isa], we present the AIWC metrics for a kernel as a Kiviat or radar diagram, for each of the problem sizes.
Unlike Shao and Brooks, we do not perform any dimensionality reduction but choose to present all collected metrics.
The ordering of the individual spokes is not chosen to reflect any statistical relationship between the metrics, however, they have been grouped into four main categories: green spokes represent metrics that indicate *parallelism*, blue spokes represent *compute* metrics, beige spokes represent *memory* metrics and purple spokes represent *control* metrics.
For clarity of visualization, we do not present the raw AIWC metrics but instead, normalize or invert the metrics to produce a scale from 0 to 1.
The parallelism metrics presented are the inverse values of the metrics collected by AIWC, i.e. **granularity** =  $1 / \textbf{work-items}$ ; **barriers per instruction** $= 1 / \textbf{mean ITB}$ ; **instructions per operand** $= 1 / \sum \textbf{SIMD widths}$.

Additionally, a common problem in parallel applications is load imbalance -- or the overhead introduced by unequal work distribution among threads.
A simple measure to quantify imbalance can be achieved using a subset of the existing AIWC metrics and is included as a further derived parallelism metric by computing **load imbalance** = **max IPT** $-$ **min IPT**.

All other values are normalized according to the maximum value measured across all kernels examined -- and on all problem sizes.
This presentation allows a quick value judgement between kernels, as values closer to the centre (0) generally have lower hardware requirements, for example, smaller entropy scores indicate more regular memory access or branch patterns, requiring less cache or branch predictor hardware; smaller granularity indicates higher exploitable parallelism; smaller barriers per instruction indicates less synchronization; and so on.

The **lud** benchmark application comprises three major kernels, **diagonal**, **internal** and **perimeter**, corresponding to updates on different parts of the matrix.
The AIWC metrics for each of these kernels are presented -- superimposed over all problem sizes -- in [Figure @fig:kiviat] A) B) and [Figure @fig:kiviat2] A) respectively.
Comparing the kernels, it is apparent that the diagonal and perimeter kernels have a large number of branch instructions with high branch entropy, whereas the internal kernel has few branch instructions and low entropy.
This is borne out through inspection of the OpenCL source code: the internal kernel is a single loop with fixed bounds, whereas diagonal and perimeter kernels contain doubly-nested loops over triangular bounds and branches which depend on thread id.
Comparing between problem sizes (moving across the page), the large problem size shows higher values than the tiny problem size for all of the memory metrics, with little change in any of the values.

The visual representation provided from the Kiviat diagrams allows the characteristics of OpenCL kernels to be readily assessed and compared.

Finally, we examine the local memory access entropy (LMAE) presented in the Kiviat diagrams in greater detail.
[Figure @fig:kiviat2] B) presents a sample of the local memory access entropy, in this instance of the LUD Perimeter kernel collected over the tiny problem size.
The kernel is launched 4 separate times during a run of the tiny problem size, this is application specific and in this instance, each successive invocation operates on a smaller data set per iteration.
Note there is a steady decrease in starting entropy, and each successive invocation of the LU Decomposition Perimeter kernel the lowers the starting entropy.
However, the descent in entropy -- which corresponds to more bits being skipped, or bigger the strides or the more localized the memory access -- shows that the memory access patterns are the same regardless of actual problem size.
In general, for cache-sensitive workloads -- such as LU-Decomposition -- a steeper descent between increasing LMAE distances indicates more localized memory accesses, and this corresponds to better cache utilisation when these applications are run on physical OpenCL devices.
It is unsurprising that applications with a smaller working memory footprint would exhibit more cache reuse with highly predictable memory access patterns.

## Use Case: AIWC analysis of OpenDwarf bioinformatics related benchmarks{#sec:case-study-bio}

A further study of the AIWC feature-space is now performed on bioinformatics type computations to show the benefits of performing AIWC analysis and a sample methodology to examine the change in AIWC metrics over a range of kernels.
The bioinformatics subset of applications from the extended OpenDwarfs benchmark suite includes computations used in sequence analysis, biophysics, gene expression/similarity and pattern identification.
`nw` and `swat` applications from the Dynamic-Programming dwarf are both directly used in sequence analysis, `gem` from the N-Body-Methods dwarf to cover biophysics computations, `hmm` from the Graphical-Models dwarf considers both sequence analysis and gene expression.
Finally, the MapReduce dwarf features the `kmeans` benchmark, which can be used directly in both pattern identification and gene similarity comparisons.
Figures\ \ref{fig:aiwc} and \ref{fig:aiwc-hmm} present radar/Kiviat diagrams of architecture-independent characteristics collected for each of the bioinformatics benchmarks.
All results are presented over a single **small** problem size, and show the multiple kernels required to compute each benchmark application as superimposed plots in the same diagram.
The **small** size was selected since `hmm`, `gem` and `swat` benchmarks are from the fixed benchmarks -- they only offer one size -- however the execution times typify those seen in the **small** sized `nw` and `kmeans` applications.

\begin{figure*}
    \centering
    \subfloat[\texttt{nw}\label{fig:aiwc-nw}]{%
        \hspace*{0.08cm}
        \includegraphics[width=0.5\textwidth]{figures/chapter-4/nw}
    }
    \subfloat[\texttt{swat}\label{fig:aiwc-swat}]{%
        \hspace*{-1.18cm}
        \includegraphics[width=0.58\textwidth]{figures/chapter-4/sw-without-setZero}
    }
    \hfill
    \subfloat[\texttt{gem}\label{fig:aiwc-gem}]{%
        \includegraphics[width=0.5\textwidth]{figures/chapter-4/gem}
    }
    \hspace*{-0.5cm}\subfloat[\texttt{kmeans}\label{fig:aiwc-kmeans}]{%
        \includegraphics[width=0.5\textwidth]{figures/chapter-4/kmeans}
    }
    %\hfill
    %\vspace*{-1.5cm}\subfloat[\texttt{hmm}\label{fig:aiwc-hmm}]{%
    %    \hspace*{-1cm}\includegraphics[width=0.7\textwidth]{figures/chapter-4/hmm}
    %}
    \caption{Architecture-Independent Workload Characterization features for selected bioinformatics benchmarks}
    \label{fig:aiwc}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{figures/chapter-4/hmm}
    \caption{Architecture-Independent Workload Characterization features for the \texttt{hmm} bioinformatics benchmark}\label{fig:aiwc-hmm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{figures/chapter-4/small-bio-run}
    \caption{EOD runtimes for {\bf small} problem sized bioinformatics benchmarks} \label{fig:small-bio-run}
\end{figure*}

The corresponding execution times of these applications is presented in Figure\ \ref{fig:small-bio-run}.
Figure\ \ref{fig:aiwc-nw} shows that the `nw` benchmark is characterized by high available thread parallelism (low values for granularity and imbalance) and a very high level of barrier synchronization.
This explains its superior performance on Nvidia GPUs compared to CPUs -- shown in Figure\ \ref{fig:small-bio-run}(a).
The Nvidia devices examined are roughly two years newer than the AMD GPUs, we expect modern AMD GPUs to form a better comparison.

Figure\ \ref{fig:aiwc-swat} shows that the `swat` benchmark also has a high level of available thread parallelism, however, it has many fewer barriers and a much higher branch entropy.
Given this, we expect to see relatively better performance on CPU architectures when examining execution times in Figure\ \ref{fig:small-bio-run}(b) -- the i7600k CPU is two years older than the optimal Nvidia GPUs presented -- it would be interesting to repeat this evaluation on a CPU of comparable vintage.

Figure\ \ref{fig:aiwc-gem} shows that the `gem` benchmark is characterized by very high available thread parallelism, and low branch and memory entropies.
This makes it ideal for GPU architectures, which is reflected in the superior performance for the modern Nvidia GPUs in Figure\ \ref{fig:small-bio-run} (c).

The `kmeans` benchmark (Figure\ \ref{fig:aiwc-kmeans}) also has a high level of available parallelism and low branch and memory entropies; as expected, the measurements in Figure\ \ref{fig:small-bio-run}(d) show that both modern GPUs and older HPC GPUs perform equally well as CPUs for this benchmark, for larger problem sizes the GPUs outperform the CPU since a sufficient amount of work has been given and is presented in the Appendix (Figure\ \ref{fig:medium-and-large-time}).

The `hmm` benchmark (Figure\ \ref{fig:aiwc-hmm}) is composed of a large number of kernels, which differ significantly in granularity.
Most kernels have very little available parallelism, suggesting that this benchmark would perform best on CPU architectures with a small number of powerful cores; this is borne out by the measurements in Figure\ \ref{fig:small-bio-run} (e) which show the smallest benchmark time was recorded on the powerful i7-6700k CPU.

None of the bioinformatics benchmarks is vectorized (instructions per operand = 1), and therefore fail to take advantage of the floating point capabilities available on CPU and MIC architectures.

## Usage and Limitations{#chapter4-aiwc-limitations}

We believe that AIWC will be useful to diversity analysis, to this end, this Section presents information about using the tool.
The AIWC plugin is only $\approx1000$ lines of code and  it is available as a fork of Oclgrind and can be publicly found on GitHub\footnote{\url{https://github.com/BeauJoh/Oclgrind}}.
To use AIWC over the command line it is passed the appropriate `--aiwc` argument immediately after calling the oclgrind program.
An example of its usage on the tiny kmeans application is shown below:

```
    oclgrind --aiwc ./kmeans -p 0 -d 0 -t 0 -- -g -p 256 -f 30
```

The collected metrics are logged as text in the command line interface during execution and also in a csv file, stored separately for each kernel and invocation.
These files can be found in the working directory with the naming convention `aiwc_`$\alpha$`_`$\beta$`.csv`.
Where $\alpha$ is the kernel name and $\beta$ is the invocation count -- the number of times the kernel has been executed.

<!--
        without AIWC                    with AIWC
        time(us)        memory(kb)      time(us)        memory(kb)
tiny    38.96           80036           73400.55        85940
small   154.93          75856           427768.6        149004
medium  2887.06         101440          12418784        636808
large   19574.41        203756          69305608        2213208
(microseconds)\label{tab:aiwc-overhead}
-->

<!--
\begin{table*}[t]
\caption{Overhead of the \textbf{AIWC} tool on the {\tt fft} benchmark and the Intel i7-6700K CPU. \label{tbl:aiwc-overhead}}
\centering
\begin{tabular}{ |l|l|l|l|l| }
\hline
        & \multicolumn{2}{|c|}{without AIWC}        & \multicolumn{2}{|c|}{with AIWC}\\
        & time ($\mu$s) & memory (\si{\kilo\byte})  & time ($\mu$s) & memory (\si{\kilo\byte}) \\
\hline
tiny    & 38.96         & 80036                     & 73400.55      & 85940 \\
small   & 154.93        & 75856                     & 427768.6      & 149004 \\
medium  & 2887.06       & 101440                    & 12418784      & 636808 \\
large   & 19574.41      & 203756                    & 69305608      & 2213208 \\
\hline
\end{tabular}
\end{table*}
-->


\begin{table*}[t]
\caption{Overhead of the \textbf{AIWC} tool on the {\tt fft} benchmark and the Intel i7-6700K CPU. \label{tbl:aiwc-overhead}}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ |l|r|r|r|r|r|r| }
\hline
        & \multicolumn{3}{c|}{time}                                         & \multicolumn{3}{c|}{memory} \\
        & \multicolumn{2}{c|}{usage (ms)}& \multicolumn{1}{c|}{increase}    & \multicolumn{2}{c|}{usage (\si{\mega\byte})}   & \multicolumn{1}{c|}{increase} \\
        & without AIWC  & with AIWC     &                                   & without AIWC   & with AIWC                     & \\
\hline
tiny    & 0.04          & 73.4          & $\approx$1830$\times$             & 80.0           &  85.9                         & 1.07$\times$        \\
small   & 0.2{ }        & 427.8         & $\approx$2800$\times$             & 75.9           &  149.0                        & 1.96$\times$        \\
medium  & 2.9{ }        & 12420{ }{ }   & $\approx$4300$\times$             & 101.4          &  636.8                        & 6.28$\times$        \\
large   & 19.6{ }       & 69300{ }{ }   & $\approx$3540$\times$             & 203.8          &  2213.2                       & 10.86$\times$       \\
\hline
\end{tabular}
}
\end{table*}


AIWC has limitations, for illustration, we examined the overheads of using AIWC on the `fft` benchmark -- from Chapter 3.
The `fft` benchmark was selected as it has average runtime results -- it falls roughly in the middle of the other benchmarks.
Table \ref{tbl:aiwc-overhead} shows the relative overhead in terms of the elapsed time per kernel invocation and the maximum resident set size (peak virtual memory usage) during the benchmark execution, the results report with and without AIWC on four sizes of the `fft` benchmark on the Intel i7-6700K CPU.
These results were collected with LibSciBench, for the kernel execution time, and Unix GNU time tool for the maximum resident set size.
The execution times are the mean time from collecting a two second sample -- the `fft` benchmark invokes the top level kernel many times during a two second run depending on problem size and the choice of OpenCL device.

We see that executing the same application on a simulator instead of directly on the Intel OpenCL runtime has significant performance costs, both in terms of execution time and memory usage.
AIWC takes 1800-4300$\times$ longer to execute depending on the problem size and uses 1.07$\times$, 1.96$\times$, 6.2$\times$, and 10.8$\times$ more memory as the problem size increases from tiny, small, medium and large respectively.
The large memory footprint was limiting for us on one of the benchmarks; we encountered an issue with running the largest `lud` application where we exhausted the available RAM on our test system (16 GB), this was overcome by running the same experiment on a system with more RAM.
The memory usage of AIWC is currently due to storing every register accessed during the simulated kernel run, it is needed for the local and global memory accesses entropy metrics which are calculated over different striding distances once the kernel has finished.
We propose instead of these addresses being stored in a linked list they instead could be written to disk -- which is much less of a factor on current development computers -- however this is future work.
Until this improvement is performed, alternatives exist if the user is running out of memory, instead of executing the full range of kernel invocations to completion -- since some applications will repeat kernel execution hundreds or thousands of times to completion -- the developer could use AIWC for performance analysis on just a few iterations or a subset of the larger problem.
In general, the final performance of the tool was not a limiting factor on a majority of the codes examined with AIWC and can still be performed quickly on the computer systems of today; \si{2{\giga\byte}} is a fraction of the total RAM available on commodity PCs, and the detailed AIWC metrics are valuable and take a few minutes to be generated on large problem sizes.

The envisaged use of AIWC is that it is only run once, for instance, a developer wished to examine the characteristics of the kernel in order to identify suitability for accelerators or verify that a high degree of SIMD vectorization had been achieved.
In the predictive scheduling setting, AIWC would be run on the codes prior to them being shipped/delivered; since these characteristics are collected by the developer on a realistic problem size, the metrics can be included as a comment in each kernels SPIR code, and the scheduler can use them by evaluating the shipped metrics on the model.
Given the proposed workflow, the overhead added by AIWC is not significant or prohibitive to the prediction and scheduler pipeline.
The predictive model is presented in detail in Chapter 5.

Examples of how AIWC metrics can be used for diversity analysis and device predictions are presented as Jupyter artefacts\footnote{\url{https://github.com/BeauJoh/aiwc-opencl-based-architecture-independent-workload-characterization-artefact}} \footnote{\url{https://github.com/BeauJoh/opencl-predictions-with-aiwc}}.


## Summary

We have presented the Architecture-Independent Workload Characterization tool (AIWC), which supports the collection of architecture-independent features of OpenCL application kernels.
It is the first workload characterization tool to support multi-threaded or parallel workloads.
These features can be used to predict the most suitable device for a particular kernel, or to determine the limiting factors for performance on a particular device, allowing OpenCL developers to try alternative implementations of a program for the available accelerators -- for instance, by reorganizing branches, eliminating intermediate variables et cetera.
In addition, the architecture independent characteristics of a scientific workload will inform designers and integrators of HPC systems, who must ensure that compute architectures are suitable for the intended workloads.

To identify which AIWC characteristics are the best indicators of opportunities for optimization, we are currently looking at how individual characteristics change for a particular code through the application of best-practice optimizations for CPUs and GPUs (as recommended in vendor optimization guides).

AIWC was also used to evaluate the performance bottlenecks of bioinformatics codes from the EOD suite.
When also coupled with the runtime performance results of Chapter 3, it is interesting to note that optimal accelerators are typically GPU based, given the high available thread parallelism and high barrier synchronization counts of many sequencing analysis applications.
However, the bioinformatics applications examined contain few kernels with higher branch and memory access entropies, interspersed with the GPU suited workloads, which suggests that CPUs are critical to achieving good performance on these systems.
Indeed, partitioning applications by scheduling kernel to their optimal accelerator may generally provide better performance for HPC bioinformatics applications.

